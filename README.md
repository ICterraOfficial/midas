![](https://img.shields.io/badge/PyTorch-1.10-green?style=plastic)
![](https://img.shields.io/badge/OpenCV-4.6-blue?style=plastic)
![](https://img.shields.io/badge/Pydicom-2.3-red?style=plastic)
![](https://img.shields.io/badge/Numpy-1.24-yellow?style=plastic)

# DoSReMC: Domain Shift Resilient Mammography Classification using Batch Normalization Adaptation

This repository was created as part of [our research](https://arxiv.org/) on batch normalization layers and its effects on the mammography classification under domain shift.

**Contents of the Repository:**

- **Data Module:** Dataset class for handling mammography images, mammography-specific transforms, and data samplers for various training strategies, including domain adversarial training. [Source code for the data module](src/data).
- **Models Module:** Contains deep learning models used in this research, along with trainer and evaluator modules for model training and evaluation in various settings, including domain adversarial training. Also includes modules for schedulers and loss functions. [Source code for the models module](src/models).
- **Utils Module:** Utility methods, such as freezing layers and plotting mammography images. [Source code for the utils module](src/utils).
- **Visualization Module:** Tensorboard modules for monitoring trainings. [Source code for the visualization module](src/visualization).
- **Notebooks:** Model training notebooks for various strategies, including domain adversarial training and training only the BN and FC layers. [Source code for the training notebooks](notebooks/).

This repository serves as a valuable resource for breast cancer recognition using mammography images. Contributions, questions, and feedback are welcome.

## Project Organization

------------

    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ .env               <- Environment variables.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
    ‚îÇ¬†¬†¬†¬† ‚îî‚îÄ‚îÄ HospitalX     <- Dataset folder.
    ‚îÇ¬†¬†¬†¬†¬†¬†  ‚îî‚îÄ‚îÄ training.xlsx   <- Metadata files contain at least the following columns: BreastID, FilePath, 
    ‚îÇ                               OneHotLabel, and ImageLaterality. For domain-adversarial training (DAT), add a DomainLabel column whose values are either Source or Target. 
    ‚îÇ¬†¬†¬†¬†¬†¬†  ‚îî‚îÄ‚îÄ validation.xlsx
    ‚îÇ¬†¬†¬†¬†¬†¬†  ‚îî‚îÄ‚îÄ test.xlsx
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks.
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
    ‚îÇ
    ‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    ‚îÇ                         generated with `pip freeze > requirements.txt`
    ‚îÇ
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data           <- Scripts to access mammography datasets, sample data, and transform images
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ DataSamplers.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Dataset.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Transforms.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ features       <- Scripts to turn raw data into features for modeling
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models         <- Scripts to implement, train and evaluate models
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ models     <- Scripts to implement models
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ EvaluationTools.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Evaluator.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ LossFunctions.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Schedulers.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Trainer.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ preprocess     <- Scripts to turn raw data into features for modeling
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ utils   <- Scripts to load configuration parameters from yaml files and to use common 
    |                  rutines of the project
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ActivationGradientHooks.py  <- Hooks for capturing activations and gradients.
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Config.py     <- Configuration class to access parameters in `config.yaml` with dot notation.
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Utils.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualization  <- Scripts for logging training results using TensorBoard.
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ Tools.py   <- Visualization methods.
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ Tensorboard.py <- Logging training events for Tensorboard.

--------


## Development Environment
For an efficient and organized development process, it is recommended to use a virtual environment. To run the code seamlessly, add the src folder to your interpreter. For users of virtualenvwrapper, run the following command in the project directory while the virtual environment is active: add2virtualenv src.



## Project Information
### Data
An in-house FFDM dataset, HCTP, along with [VinDr-Mammo](https://doi.org/10.1038/s41597-023-02100-7) and [CSAW-CC (mammography)](https://doi.org/10.5878/45vm-t798), were used. The clinical data used in this study are not publicly available due to institutional data ownership and confidentiality policies. Access to the data may be considered on reasonable request and with permission from the corresponding institutional authorities.
<br>

### Results
#### ROC- and PR-AUC Scores
<div align="center">
    <img src="reports/figures/results.png" alt="Results" width="750"  height="550" />
    <p>Table 2: Models follow the notation <code>ùìú<sub>source‚Üítarget</sub><sup>statistics</sup></code> where <em>source</em> is the training dataset, <em>target</em> is the evaluation dataset, and the superscript <em>statistics</em> indicates the BN statistics used: <em>tr</em> for training-time moving averages, <em>tt</em> for test-time recomputed statistics. The model denoted with an apostrophe (<code>'</code>) indicates evaluation conducted on input data that has been normalized to the [0, 1] range.</p>
</div>
<br>

#### Appendix A. Kernel Density Estimation (KDE)
<div align="center">
    <img src="reports/figures/kde_l1b2bn2.png" alt="ResNet Layer: 1, Block: 2, BN: 2" width="600"  height="250" />
</div>

<div align="center">
    <img src="reports/figures/kde_l3b2bn2.png" alt="ResNet Layer: 3, Block: 2, BN: 2" width="600"  height="550" />
</div>

<div align="center">
    <img src="reports/figures/kde_l4b2bn2.png" alt="ResNet Layer: 4, Block: 2, BN: 2" width="600"  height="850" />
    <p>Figure A.13: KDEs of per-channel activations for BN layers in the second block of ResNet layers 2, 3, and 4. All KDEs in this section are computed using a mini-batch of 16 images sampled from the HCTP dataset. </p>
</div>

## Reference

If you find this work useful, please cite our paper:

    @article{aky√ºz2025dosremc, 
    title={DoSReMC: Domain Shift Resilient Mammography Classification using Batch Normalization Adaptation},
    author={Aky√ºz, Uƒüurcan and Katircioglu-√ñzt√ºrk, Deniz and S√ºsl√º, Emre K and Kele≈ü, Burhan and Kaya, Mete C and Durhan, Gamze and Akpƒ±nar, Meltem G and Demirkazƒ±k, Figen B and Akar, G√∂zde B},
    journal={arXiv preprint arXiv:2508.15452},
    year={2025},
    doi={10.48550/arXiv.2508.15452}

<br><br><br>
<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>

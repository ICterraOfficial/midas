{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ada5430127d6e0",
   "metadata": {},
   "source": [
    "## Info\n",
    "This notebook is designed for hyperparameter search. The experimental configurations are loaded from the `config.yaml` file, and the training setup is initialized accordingly. Note that absolute paths of the dataset files and model weights should be used in the `config.yaml` file.\n",
    "\n",
    "During the training process:\n",
    "- Ray library is used.\n",
    "- The training configuration file (`config.yaml`) is copied to the output directory for reference.\n",
    "- Search progress can be monitored in the notebook.\n",
    "- It could be useful to implement your custom data sampling procedure to speed up hyperparameter search.\n",
    "- You can find more details about Ray Tune [here](https://docs.ray.io/en/latest/tune/index.html).\n",
    "\n",
    "The setup ensures that key information related to hyperparameter search is easily accessible and logged for future analysis and model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141c817-89d0-4c4d-b6d3-d83ca4150057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import RunConfig\n",
    "from ray.air import session\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as tt\n",
    "\n",
    "from data import ClassificationDataset\n",
    "from data import Transforms as T\n",
    "from models import Evaluator, GMIC, GMICLoss, Trainer\n",
    "from utils import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605001536c59fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd7df4b28dc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_subset(metadata, n_breastid=100):\n",
    "    # Implement your custom sampling algoritm.\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed993692245a3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(cfg):\n",
    "    \"\"\"\n",
    "    Creates and returns training and validation datasets with specified transformations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : Config\n",
    "        Configuration object containing dataset paths and parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing (train_dataset, val_dataset), where each is a ClassificationDataset\n",
    "        with appropriate transforms applied.\n",
    "    \"\"\"\n",
    "    \n",
    "    transform_train = {'dicom': None, 'pytorch': None}\n",
    "    transform_val = {'dicom': None, 'pytorch': None}\n",
    "\n",
    "    transform_train['dicom'] = [T.UIntToFloat32(), T.StandardScoreNormalization(),\n",
    "                                T.RandomGaussianNoise(mean=.0, std=.005)]\n",
    "\n",
    "    transform_train['pytorch'] = tt.Compose([tt.RandomHorizontalFlip(p=0.5),\n",
    "                                             tt.RandomRotation([-15, +15]),\n",
    "                                             tt.RandomAffine(degrees=0, translate=(0, 0.1), shear=(-25, +25)),\n",
    "                                             tt.RandomResizedCrop((cfg.data.inp_height, cfg.data.inp_width),\n",
    "                                                                  scale=(0.8, 1.6))])\n",
    "\n",
    "    transform_val['dicom'] = [T.UIntToFloat32(), T.StandardScoreNormalization()]\n",
    "\n",
    "    train = ClassificationDataset(cfg.data.train_xlsx_path, transform=transform_train)\n",
    "    val = ClassificationDataset(cfg.data.val_xlsx_path, transform=transform_val)\n",
    "\n",
    "    # train.metadata = sample_subset(train.metadata, n_breastid=250)\n",
    "\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf84170eae18306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crate_dataloaders(cfg, train, val):\n",
    "    \"\"\"\n",
    "    Creates data loaders for training and validation datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : Config\n",
    "        Configuration object containing batch size and other parameters\n",
    "    train : Dataset\n",
    "        Training dataset\n",
    "    val : Dataset\n",
    "        Validation dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (train_loader, val_loader) - Tuple containing training and validation DataLoader objects\n",
    "    \"\"\"\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=cfg.data.batch_size, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val, batch_size=cfg.data.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c4f351e92adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(cfg):\n",
    "    \"\"\"\n",
    "    Creates the model, loads pretrained weights and moves it to the specified device defined in the config file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : Config\n",
    "        Configuration object containing model parameters and paths\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model\n",
    "        Initialized the model with loaded weights on the specified device\n",
    "    \"\"\"\n",
    "\n",
    "    model = GMIC(cfg.gmic_parameters)\n",
    "    pretrained_model = torch.load(cfg.model.weight_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(pretrained_model, strict=False)\n",
    "    model.to(cfg.gmic_parameters.device_type)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbdfeb0f128a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(config, training_cfg_path):\n",
    "    \"\"\"\n",
    "    Training objective function for hyperparameter optimization using Ray Tune.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary containing the hyperparameters to optimize:\n",
    "    training_cfg_path : str\n",
    "        Path to the training configuration YAML file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Results are reported to Ray Tune via session.report()\n",
    "    \"\"\"\n",
    "        \n",
    "    cfg = Config(training_cfg_path)\n",
    "    cfg['train']['lr'] = config['lr']\n",
    "    cfg['train']['beta'] = config['beta']\n",
    "\n",
    "    \n",
    "    train, val = get_dataset(cfg)\n",
    "    train_loader, val_loader = crate_dataloaders(cfg, train, val)\n",
    "    model = get_model(cfg)\n",
    "    \n",
    "    # Set other model components.\n",
    "    criterion = GMICLoss(beta=cfg.train.beta)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.train.lr, weight_decay=0.001)\n",
    "    \n",
    "    trainer = Trainer(criterion=criterion, model=model, optimizer=optimizer, \n",
    "                      total_epochs=cfg.train.epoch, data_loader=train_loader)\n",
    "    evaluator = Evaluator(model=model, data_loader=val_loader)\n",
    "        \n",
    "    for epoch in range(0, cfg.train.epoch): \n",
    "        train_metrics = trainer.fit(prog_bar=False)\n",
    "        val_metrics = evaluator.evaluate(prog_bar=False)\n",
    "\n",
    "        metrics = {'train/roc_auc': train_metrics['roc']['auc'],\n",
    "                   'train/pr_auc': train_metrics['pr']['auc'],                  \n",
    "                   'total_loss': train_metrics['total_loss'],  \n",
    "                   'val/roc_auc': val_metrics['roc']['auc'],\n",
    "                   'val/pr_auc': val_metrics['pr']['auc']                            \n",
    "        }\n",
    "\n",
    "        tempdir = os.path.join(session.get_trial_dir(), 'checkpoint')\n",
    "        os.makedirs(tempdir, exist_ok=True)\n",
    "        torch.save({'epoch': epoch, \n",
    "                    'model_state': model.state_dict()},\n",
    "                    os.path.join(tempdir, 'checkpoint.pt'))\n",
    "            \n",
    "        # Send the current training result back to Tune.\n",
    "        session.report(metrics=metrics, checkpoint=Checkpoint.from_directory(tempdir))\n",
    "        # session.report(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac16000672fe01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes the Ray Dashboard accessible from any IP address.\n",
    "ray.init(dashboard_host='0.0.0.0')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa1b3c064fee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the search is interrupted, set resume=True to continue from the last checkpoint.\n",
    "resume = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eda4342d35cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration path. \n",
    "# !!! Do not use absolute paths in the config file.\n",
    "training_cfg_path = '/home/user/project_name/notebooks/batch_norm_experiment/config.yaml'\n",
    "\n",
    "# Output path of ray results.\n",
    "storage_path = '/home/user/project_name/models/hparam_search/'\n",
    "\n",
    "# Define experiment name. Date will be an identifier for hyperparameter optimization experiments.\n",
    "date = str(datetime.date.today())\n",
    "date = date.replace('-', '_')\n",
    "experiment_name = '{}_lr.beta_search'.format(date)\n",
    "\n",
    "# Define hyperparameter search space.\n",
    "search_space = {\n",
    "    'lr':   tune.loguniform(10**-5.5, 10**-4),\n",
    "    'beta': tune.loguniform(10**-5.5, 10**-3.5)\n",
    "}\n",
    "\n",
    "# Trainable method for ray tune.\n",
    "trainable_with_gpu = tune.with_resources(tune.with_parameters(objective, training_cfg_path=training_cfg_path), \n",
    "                                         resources={'cpu':2, 'gpu': 2})\n",
    "\n",
    "# Hyperparameter search algorithm grid, random, Bayesian Optimization\n",
    "bayesopt = None # BayesOptSearch(metric='train/pr_auc', mode='max')\n",
    "\n",
    "# Tune configuration\n",
    "tune_config = tune.TuneConfig(num_samples=50, scheduler=ASHAScheduler(metric='val/pr_auc', \n",
    "                                                                     mode='max', grace_period=10), search_alg=bayesopt)\n",
    "\n",
    "# Run configuration\n",
    "run_config = RunConfig(name=experiment_name, storage_path=storage_path)\n",
    "\n",
    "if not resume:\n",
    "    tuner = tune.Tuner(trainable_with_gpu,\n",
    "                       tune_config=tune_config,                  \n",
    "                       param_space=search_space,\n",
    "                       run_config=run_config,\n",
    "                       )\n",
    "    \n",
    "    # Copy training configuration like dataset file paths, model path, etc. into this folder too.\n",
    "    shutil.copy2(training_cfg_path, os.path.join(storage_path, experiment_name))\n",
    "else:\n",
    "    tuner = tune.Tuner.restore(path=os.path.join(storage_path, experiment_name),\n",
    "                              trainable=trainable_with_gpu)\n",
    "\n",
    "# Start hyperparameter search.\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13ec00c51462b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the experiment path to load data of all experiments\n",
    "experiment_path = os.path.join(storage_path, experiment_name)\n",
    "\n",
    "# Load analysis from the results directory\n",
    "analysis = tune.ExperimentAnalysis(experiment_path)\n",
    "\n",
    "# Set the columns to be visualized\n",
    "columns = ['config/lr', 'config/beta', 'val/roc_auc', 'val/pr_auc', 'train/roc_auc', 'train/pr_auc', 'total_loss']\n",
    "\n",
    "# Create the DataFrame and sort based on the metric\n",
    "all_trials = pd.concat(analysis.trial_dataframes.values(), ignore_index=True)\n",
    "all_trials = all_trials.sort_values('val/pr_auc', ascending=False)[columns]\n",
    "\n",
    "# See the best results\n",
    "all_trials.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7d0b5-bf45-4568-9b9b-e2cf7cc9554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See exact values\n",
    "print('LR of best result: {}'.format(all_trials.iloc[0, 0]))\n",
    "print('Beta of best_result: {}'.format(all_trials.iloc[0, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midas_au_hparam",
   "language": "python",
   "name": "midas_au_hparam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

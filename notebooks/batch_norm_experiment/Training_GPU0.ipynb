{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d26714d-ede6-4dce-8a07-ca0ed89b0bcd",
   "metadata": {},
   "source": [
    "## Info\n",
    "This notebook is designed for model training. The experimental configurations are loaded from the `config.yaml` file, and the training setup is initialized accordingly.\n",
    "\n",
    "During the training process:\n",
    "- Metrics are logged using `TensorBoardLogger` and saved under the specified `output_path`.\n",
    "- The training configuration file (`config.yaml`) is copied to the output directory for reference.\n",
    "- Model weights are also saved in the output directory after training.\n",
    "\n",
    "The setup ensures that key information related to training is easily accessible and logged for future analysis and model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa4eca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:25:19.130395Z",
     "start_time": "2025-04-23T14:25:15.843054Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "from data import ClassificationDataset\n",
    "from data import Transforms as T\n",
    "from models import Evaluator, GMIC, GMICLoss, Trainer\n",
    "from visualization import TensorboardLogger\n",
    "from utils import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e968b",
   "metadata": {},
   "source": [
    "### Experiment configuration"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For reproducibility, set the random seeds for torch and the Python random module.\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Prefix the experiment name with todayâ€™s date.\n",
    "date = str(datetime.date.today())\n",
    "date = date.replace('-', '_')\n",
    "\n",
    "# Path to save the experiment logs and outputs.\n",
    "output_path = '../../models/Experiment1/{}_HospitalA'.format(date)\n",
    "weight_path = os.path.join(output_path, \"weights/\")"
   ],
   "id": "1084cdfae246eff1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the configuration as a Python object for easy access to parameters.\n",
    "# For example, the batch size can be accessed using dot notation:\n",
    "# print(cfg.data.batch_size)\n",
    "# > 8\n",
    "\n",
    "cfg_path = 'config.yaml'\n",
    "cfg = Config(cfg_path)"
   ],
   "id": "2d395438296f482"
  },
  {
   "cell_type": "markdown",
   "id": "24a5c0f8",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the transformations used for training and validation.\n",
    "# The 'pytorch' key specifies transformations from the official PyTorch library,\n",
    "# while the 'dicom' key refers to custom transformations developed for mammography images.\n",
    "transform_train = {'dicom': None, 'pytorch': None}\n",
    "transform_val = {'dicom': None, 'pytorch': None}\n",
    "\n",
    "transform_train['dicom'] = [# T.FlipToLeft(), T.CropBreastRegion(),\n",
    "                            # T.Resize(height=cfg.data.inp_height, width=cfg.data.inp_width),\n",
    "                            T.UIntToFloat32(), T.StandardScoreNormalization(),\n",
    "                            T.RandomGaussianNoise(mean=.0, std=.005)]\n",
    "\n",
    "transform_train['pytorch'] = tt.Compose([tt.RandomHorizontalFlip(p=0.5),\n",
    "                                         tt.RandomRotation([-15, +15]),\n",
    "                                         tt.RandomAffine(degrees=0, translate=(0,0.1), shear=(-25, +25)),\n",
    "                                         tt.RandomResizedCrop((cfg.data.inp_height, cfg.data.inp_width), scale=(0.8, 1.6))])\n",
    "\n",
    "transform_val['dicom'] = [# T.FlipToLeft(), T.CropBreastRegion(),\n",
    "                          # T.Resize(height=cfg.data.inp_height, width=cfg.data.inp_width),\n",
    "                          T.UIntToFloat32(), T.StandardScoreNormalization()]\n",
    "\n",
    "# Add data augmentations to the config file for logging and reproducibility during experiments.\n",
    "cfg['data']['transforms'] = f\"training={[str(transform) for transform in transform_train['dicom']]}\" + \\\n",
    "                            ' * ' + f\"{[str(transform) for transform in transform_train['pytorch'].transforms]}\" + \\\n",
    "                            ' | ' + f\"validation={[str(transform) for transform in transform_val['dicom']]}\"\n",
    "\n",
    "# Create dataset objects using the Classification class.\n",
    "# Each dataset returns breast_id, image, label, and optionally a domain_label.\n",
    "# The metadata of the dataset object can also be viewed using `print(dataset.metadata)`.\n",
    "train = ClassificationDataset(metadata_path=cfg.data.train_xlsx_path, transform=transform_train)\n",
    "val = ClassificationDataset(metadata_path=cfg.data.val_xlsx_path, transform=transform_val)"
   ],
   "id": "a9a66e65206169ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataloader",
   "id": "72778820ac8748a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define dataloaders for training and validation sets.\n",
    "train_loader = DataLoader(train, batch_size=cfg.data.batch_size, \n",
    "                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val, batch_size=cfg.data.batch_size, \n",
    "                        shuffle=True, num_workers=4, pin_memory=True)"
   ],
   "id": "96e0225560abf3a8"
  },
  {
   "cell_type": "markdown",
   "id": "47e7e958",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create the Tensorboard object to save experiment outputs.\n",
    "tb = TensorboardLogger(output_path)"
   ],
   "id": "235c1598196748d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize the model object.\n",
    "model = GMIC(cfg.gmic_parameters)"
   ],
   "id": "1b42cba1c5ffa21f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b9726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If pretrained model weights are defined in the config, load them.\n",
    "if cfg.model.weight_path:\n",
    "    weights = torch.load(cfg.model.weight_path, map_location=torch.device('cpu'))\n",
    "    # Skip loading the 'shared_rep_filter' key by setting strict=False.\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "    print('Model weights are loaded!')\n",
    "# Send the model to the device specified in `config.yaml`, either 'cuda' or 'cpu'.\n",
    "model = model.to(cfg.gmic_parameters.device_type)\n",
    "\n",
    "# Log the model architecture to TensorBoard's graph.\n",
    "tb.add_graph(model, (1, cfg.data.inp_height, cfg.data.inp_width), cfg.gmic_parameters.device_type)\n",
    "tb.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e3e49",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function (criterion) and optimizer.\n",
    "criterion = GMICLoss(beta=cfg.train.beta)\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.train.lr, weight_decay=0.001)\n",
    "\n",
    "# Log the loss function and optimizer along with their parameters for reproducibility.\n",
    "cfg['train']['LossFunction'] = str(criterion)\n",
    "cfg['train']['Optimizer'] = str(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer object. The training strategy and loop are defined within this class\n",
    "# based on the objects created above.\n",
    "trainer = Trainer(criterion=criterion, model=model, optimizer=optimizer, \n",
    "                  total_epochs=cfg.train.epoch, data_loader=train_loader)\n",
    "\n",
    "# Initialize the evaluator object. The evaluation strategy and loop are defined within this class\n",
    "# based on the objects created above.\n",
    "evaluator = Evaluator(model=model, data_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2397fa",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the weight folder along with the output folder if they do not exist.\n",
    "if not os.path.isdir(weight_path):\n",
    "    os.makedirs(weight_path)\n",
    "\n",
    "# Save the `config.yaml` file into the experiment folder.\n",
    "# This file can be used for evaluating and investigating the experiment later.\n",
    "cfg.save(os.path.join(output_path, 'config.yaml'))\n",
    "\n",
    "# The 'config.to_markdown()' function adds the `config.yaml` context as markdown in TensorBoard. \n",
    "# You can view it under the Text tab in TensorBoard.\n",
    "tb.add_text('HyperParameters', cfg.to_markdown())\n",
    "tb.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62652c0-2229-49e6-9f9e-e40e4b979fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the starting point for the PR AUC (Precision-Recall AUC).\n",
    "prev_pr_auc = .0"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This is the highest level of the training loop, defining the number of epochs.\n",
    "# At each epoch, the model is trained for one iteration, and the trained model is evaluated.\n",
    "# Collected metrics are logged to TensorBoard, and the model weights are saved to the output folder.\n",
    "\n",
    "for epoch in range(0, cfg.train.epoch):\n",
    "    # Exclusively assign the current epoch to the trainer object.\n",
    "    # This is used to monitor the progress bar and is also helpful for schedulers\n",
    "    # that need to track the current epoch during training.\n",
    "    trainer.curr_epoch = epoch\n",
    "    # Before iterating over the training dataset, get the learning rate for logging purposes.\n",
    "    # Schedulers may update the learning rate after the training loop, \n",
    "    # which could cause it to reflect an incorrect value for the current epoch.\n",
    "    # To avoid this inconsistency, retrieve the learning rate before calling `fit()`\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    train_metrics = trainer.fit()\n",
    "    \n",
    "    # Add results to the Tensorboard.\n",
    "    tb.add_scalars(step=epoch+1, lr=curr_lr, train_loss=train_metrics['total_loss'],\n",
    "                  roc_auc=train_metrics['roc']['auc'], pr_auc=train_metrics['pr']['auc'], data_split='Train')\n",
    "    tb.flush()\n",
    "    \n",
    "    # Evaluate the model on the validation set. \n",
    "    # It is configured to evaluate at the end of every epoch with a frequency of '1'.\n",
    "    if epoch % 1 == 0:\n",
    "        val_metrics = evaluator.evaluate()\n",
    "        tb.add_scalars(step=epoch+1, roc_auc=val_metrics['roc']['auc'], \n",
    "                       pr_auc=val_metrics['pr']['auc'], data_split='Val')\n",
    "        tb.flush()\n",
    "\n",
    "    # Save the currently trained model as 'last_model'.\n",
    "    # If any issues occur during training, update 'prev_pr_auc' with the last PR-AUC score\n",
    "    # update to current epoch in 'range(0, cfg.train.epoch)'\n",
    "    # and 'cfg.model.weight_path' with the last saved model. \n",
    "    # Then, rerun the notebook.\n",
    "    trainer.save_model('{0}/last_model.pth'.format(weight_path))\n",
    "    # If the current model yields a better PR-AUC, save it as 'best_model.pth' in the `weights` folder.\n",
    "    if val_metrics['pr']['auc'] > prev_pr_auc:\n",
    "        prev_pr_auc = val_metrics['pr']['auc']\n",
    "        trainer.save_model('{0}/best_model.pth'.format(weight_path))\n",
    "    # Save the model as a checkpoint every 10 epochs.\n",
    "    if epoch % 10 == 0:\n",
    "        trainer.save_model('{0}/{1}_model.pth'.format(weight_path, epoch))\n",
    "            \n",
    "# Save the last model too.\n",
    "trainer.save_model('{0}/last_model.pth'.format(weight_path))\n",
    "tb.close()"
   ],
   "id": "be46824137dc4c1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Release GPU memory after training ends. \n",
    "# Note that this may not release all GPU memory.\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "d3269820b291492e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midas_github",
   "language": "python",
   "name": "midas_github"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
